
## 모션인식 코드 수정 for 성능개선

### 문제점

- 동영상 매 프레임 마다 골격 데이터를 추출하며 약 20초 가량의 동영상에서 1만개 이상의 골격 데이터가 추출된다.
- 또한, 웹소켓으로 client 이미지를 server로 전송하고 모션인식 모델을 돌리고난 후, 결과 이미지를 전송해 웹 페이지에 띄울 때 속도가 현저히 느려졌다.

### 개선방향

- 현재 동영상 모든 프레임의 골격 데이터를 검출하고 있어 프레임 수를 조절하여 골격 데이터를 추출하는 방향으로 코드 수정

### 해결

- 영상에서 프레임을 읽어오는 while 문안에서 지정된 FPS 이후 프레임을 받아오는 방식으로 코드를 수정
- [참고블로그](https://deep-eye.tistory.com/10)

```python
def dance_video_processing(video_path= r'dance_video/correct30.mp4',showBG = True):

    cap = cv2.VideoCapture(video_path)
    prev_time = 0
    FPS = 10

    while True:
        ret_val, image = cap.read()
        current_time = time.time() - prev_time
        dim = (368, 428)
        if (ret_val is True) and (current_time > 1./FPS) :
            
						...

            prev_time = time.time() 

		        ...
          
        elif (ret_val is False) :
            break
        else:
            cv2.waitKey(1)
            # 키보드를 기다리는 함수,
            # 영상을 연속적으로 재생할 것이기 때문에
            # 입력 대기 시간을 1ms 로 지정하여 입력에 관계없이 1ms 마다 새롭게 갱신 
   
    cap.release()
    cv2.destroyAllWindows()
    return keypoints_list
```

### 기타 시도 방법

- 동시에 동영상 입력 받고 모션인식 후, 값 반환
    - `compare_positions` class 내에서 두 영상(정답 영상, 회원영상) 동시에 골격 데이테를 추출한다.
    - 결과
        - 통신없이 하나의 python 파일 내에서 돌렸는데도 속도가 느려 부적합한 방법이라 판단.
    
    ```python
    def compare_positions(trainer_video,user_video,keyp_list, dim=(420,720)):
        cap = cv2.VideoCapture(trainer_video)
        cam = cv2.VideoCapture(user_video) 
        cam.set(3, w)
        cam.set(4, h)
        # fps_time = 0 #Initializing fps to 0
        prev_time = 0
        FPS = 1
        while True:
            ret_val, image_1 = cam.read()
            e_d=0
            ret_val_1,image_2 = cap.read()
            current_time = time.time() - prev_time
            if ret_val_1 and ret_val:
                image_1 = cv2.flip(image_1, 1)
                
                # resizing the images
                image_1 = cv2.resize(image_1, dim, interpolation = cv2.INTER_AREA)
                image_2 = cv2.resize(image_2, dim, interpolation = cv2.INTER_AREA)
                
                humans_2 = e.inference(image_1, resize_to_default=(w > 0 and h > 0),upsample_size=4.0 )
                
                #Dancer keypoints and normalization
                transformer = Normalizer().fit(keyp_list)  
                keyp_list=transformer.transform(keyp_list)
                # Showing FPS
                cv2.putText(image_2, "FPS: %f" % (1.0 / (time.time() - prev_time)), (10, 10),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
               
                # Getting User keypoints, normalization and comparing also plotting the keypoints and lines to the image
                image_1 = TfPoseEstimator.draw_humans(image_1, humans_2, imgcopy=False) # 선 표시하는 코드
    
                # Displaying the dancer feed.
                cv2.imshow('Dancer Window', image_2)
                npimg = np.copy(image_1)
                image_h, image_w = npimg.shape[:2]
                centers = {}
                keypoints_list=[]
                for human in humans_2:
                              # draw point
                        for i in range(common.CocoPart.Background.value):
                                    if i not in human.body_parts.keys():
                                            continue
    
                                    body_part = human.body_parts[i]
                                    x_axis=int(body_part.x * image_w + 0.5)
                                    y_axis=int(body_part.y * image_h + 0.5)
                                    center=[x_axis,y_axis]
                                    centers[i] = center
                        k=-2
                        features=[0]*36
                        for j in range(0,18):
                            k=k+2
                            try:
                                if k>=36:
                                    break
                                #print(k)
                                #print(keypoints_list[i][j])
                                features[k] = centers[j][0]
                                features[k+1] = centers[j][1]
                            except:
                                features[k]=0
                                features[k+1]=0
                        features=transformer.transform([features])
                        #print(features[0])
                        min_=100 # Intializing a value to get minimum cosine similarity score from the dancer array list with the user
                        for j in keyp_list:
                            print(j, ", ", features[0])
                            sim_score=findCosineSimilarity_1(j,features[0])
                            print(sim_score)
                            #print(sim_score)
                            #Getting the minimum Cosine Similarity Score
                            if min_>sim_score:
                                min_=sim_score
                # Displaying the minimum cosine score
                cv2.putText(image_1, str(min_), (10, 30),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)
                # If the disctance is below the threshold
                if min_<0.15:
                    cv2.putText(image_1, "CORRECT STEPS", (120, 700),
                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
                else:
                    cv2.putText(image_1,  "NOT CORRECT STEPS", (80, 700),
                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
                cv2.putText(image_1, "FPS: %f" % (1.0 / (time.time() - prev_time)), (10, 50),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
                # Display the user feed
                
                cv2.imshow('User Window', image_1)
                prev_time = time.time()
                
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    break
            else:
                break
    
        cam.release()
        cap.release()
        cv2.destroyAllWindows()
    ```