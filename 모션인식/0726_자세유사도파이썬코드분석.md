# 1. 초기 세팅

### 크기 설정 및 모델 설정

```python
resize = '432x368'     # resize images before they are processed
resize_out_ratio = 4.0 # resize heatmaps before they are post-processed
model='mobilenet_v2_large'
```

#### 🔎 mobilenet_v2_large

- 기존의 mobilenet v1에서는 `Depthwise Separable Convolution` 을 도입하여 연산량과 모델 사이즈를 줄일 수 있어서, 모바일 디바이스와 같은 베한된 환경에서도 사용하기에 적합한 NN
- 이 모델의 핵심 개념은 `Inverted Residual` 구조
- 정확한 원리들은 뭔말인지 모르겠고, `학습을 위한 모델`이라고 생각하면 될 듯.

---

#### Estimator 구축

```python
from tf_pose.estimator import TfPoseEstimator
from tf_pose.networks import get_graph_path, model_wh

w, h = model_wh(resize)
if w > 0 and h > 0:
    e = TfPoseEstimator(get_graph_path(model), target_size=(w, h), trt_bool=False)
else:
    e = TfPoseEstimator(get_graph_path(model), target_size=(432, 368), trt_bool=False)
print('********* Model Ready *************')
```



# 2. 댄서의 자세 추출

- 트레이너의 키포인트를 추출해서 리스트에 저장

- `dance_video_processing`: 비디오에서 keypoints을 추출

  - input
    - video_path: 비디오 경로
    - showBG: 영상을 백그라운드로 출력 유무 (기본값: True)
  - return
    - keypoints_list: 트레이너의 키포인트 리스트

  ```python
  def dance_video_processing(video_path= r'dance_video/dancer.mp4',showBG = True):
          cap = cv2.VideoCapture(video_path)
          if cap.isOpened() is False:
              print("Error opening video stream or file")
          fps_time = 0
          while True:
              ret_val, image = cap.read()
              dim = (368, 428)
              if ret_val:
                   # resize image
                  image = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)
                  humans = e.inference(image,
                                       resize_to_default=(w > 0 and h > 0),
                                       upsample_size=4.0)
                  if not showBG:
                      image = np.zeros(image.shape)
                  # Plotting the keypoints and lines to the image 
                  image = TfPoseEstimator.draw_humans(image, humans, imgcopy=False)
                  npimg = np.copy(image)
                  image_h, image_w = npimg.shape[:2]
                  centers = {}
                  keypoints_list=[]
                  for human in humans:
                            # draw point
                          for i in range(common.CocoPart.Background.value):
                                  if i not in human.body_parts.keys():
                                          continue
  
                                  body_part = human.body_parts[i]
                                  x_axis=int(body_part.x * image_w + 0.5)
                                  y_axis=int(body_part.y * image_h + 0.5) 
                                  center=[x_axis,y_axis]
                                  centers[i] = center
                                  keypoints_list.append(centers)
                  # To display fps
                  cv2.putText(image, "FPS: %f" % (1.0 / (time.time() - fps_time)), (10, 10),
                              cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
                  # To display image
                  cv2.imshow('Dancer', image)
                  fps_time = time.time()
                  if cv2.waitKey(1) & 0xFF == ord('q'):
                      break
                  
              else:
                  break
          #print(keypoints_list)
          cap.release()
          cv2.destroyAllWindows()
          return keypoints_list
  ```

  