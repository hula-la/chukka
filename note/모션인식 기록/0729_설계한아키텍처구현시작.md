## 1. HTTP 통신 - 강사 영상

https://stribny.name/blog/fastapi-video/

##### FrontEnd

```html
<!DOCTYPE html>
<html>
    <head>
        <title>FastAPI video streaming</title>
    </head>
    <body>
        <video width="1200" controls muted="muted">
            <source src="http://localhost:8000/video" type="video/mp4" />
        </video>
    </body>
</html>
```



---

##### 	websocket이 아닌 실시간 영상 스트리밍

​	https://access-violation.tistory.com/17

---

##### - 공식문서 참고해서 영상 보내기 성공

- **html**: `index_2.html`
- **fastAPI**: `fastapi_prac.py`

https://fastapi.tiangolo.com/ko/advanced/custom-response/?h=video#using-streamingresponse-with-file-like-objects

```python
from fastapi import FastAPI
from fastapi.responses import StreamingResponse

some_file_path = "large-video-file.mp4"
app = FastAPI()


@app.get("/")
def main():
    def iterfile():  # 
        with open(some_file_path, mode="rb") as file_like:  # 
            yield from file_like  # 

    return StreamingResponse(iterfile(), media_type="video/mp4")
```

- 공식문서에서 이 외의 코드들 중에서도 참고할 코드 있는지 보기
- 기존의 웹소켓통신도 HTTP로 가능한지 보기

---

## 2. 싱크 맞추기

#### 기존의 강사 자세 데이터 추출 코드 개선

##### a. 깃폴더에 가상환경이랑 전체 옮김

- `No module named '_pafprocess'
  you need to build c++ library for pafprocess. See : https://github.com/ildoonet/tf-pose-estimation/tree/master/tf_pose/pafprocess`

  에러가 떠서 아래 명령으로 환경설정

  ```bash
  cd tf_pose/pafprocess
  swig -python -c++ pafprocess.i && python setup.py build_ext --inplace
  ```

  

##### b. OpenCV API 서칭 (https://deepflowest.tistory.com/107)

- ##### 동영상 및 카메라 프레임 읽기

  - `cap = cv2.VideoCaputure(file_path 또는 index)` : 비디오 캡처 객체 생성

    - file_path : 동영상 파일 경로
    - index : 카메라 장치 번호 (0 부터 차례로 증가)
    - cap : VideoCapture 객체

    - `ret = cap.isOpened()` : 객체 초기화 확인
      - ret : 초기화 여부, True/False

    - `ret, img = cap.read() : 영상 프레임 읽기`
      - ret : 프레임 읽기 성송 또는 실패 여부, True / False
      - img : 프레임 이미지, Numpy 배열 또는 None

  - `cap.set(id, value)` : 프로퍼티 변경

  - `cap.get(id)` : 프로퍼티 확인

  - `cap.release()` : 객체 자원 반납

- ##### 카메라 비디오 속성 제어

  - cv2.CAP_PROP_FRAME_WIDTH : 프레임 폭
  - cv2.CAP_PROP_FRAME_HEIGHT : 프레임 높이
  - cv2.CAP_PROP_FPS : 프레임 초당 프레임 수
  - cv2.CAP_PROP_POS_MSEC : 동영상 파일의 프레임 위치(MS)
  - cv2.CAP_PROP_POS_AVI_RATIO : 동영상 파일의 상대 위치 (0:시작 , 1:끝)
  - cv2.CAP_PROP_FOURCC : 동영상 파일 코덱 문자
  - cv2.CAP_PROP_AUTOFOCUS : 카메라 자동 초점 조절
  - cv2.CAP_PROP_ZOOM : 카메라 줌

- ##### FPS를 지정해서 동영상 재생

  - 동영상의 FPS를 구하고 다음과 같이 적절한 지연 시간을 계산해서 지정할 수 있다.

    ```python
    fps = cap.get(cv2.CP_PROP_FPS)
    delay = int(1000/fps)
    ```

    ```python
    import cv2
    
    video_file = "img/a.mp4"
    
    cap = cv2.VideoCapture(video_file)
    if cap.isOpened():
    	fps = cap.get(cv2.CAP_PROP_FPS)
        delay = int(1000/fps)
        print("FPS: %f, Delay: %dms" %(fps, delay))
        
        while True:
            ret, img = cap.read()
            if ret:
                cv2.imshow(video_file, img)
                cv2.waitKey(delay)
            else:
                break
    else:
    	print("can't open video")
    cap.release()
    cv2.destroyAllWindows()
    ```

##### c. 코드 수정

- 기존엔 강사 자세 좌표 갯수가 실제 추출되어야할 프레임 수와 달랐음. 3400정도
  - 코드 분석 후 수정 완료 
    - 영상길이 21초
    - FPS 10 설정
    - **추출된 프레임 갯수 214** 👍

- `dance_video_processing`

  ```python
  def dance_video_processing(video_path= r'dance_video/correct30.mp4',showBG = True):
  
      cap = cv2.VideoCapture(video_path)
      video_fps = cap.get(cv2.CAP_PROP_FPS)
      # delay 추가, 실제 비디오 fps로 조절함
      delay = int(1000/video_fps)
  
      if cap.isOpened() is False:
          print("Error opening video stream or file")
  
      # prev_time = 0 -> 맨처음 시간 초기화 수정
      prev_time = time.time()
      FPS = 10
      keypoints_list=[]
      
      while True:
          ret_val, image = cap.read()
          current_time = time.time() - prev_time
          dim = (368, 428)
          if (ret_val is True) and (current_time > 1./FPS) :
              image = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)
              humans = e.inference(image,
                                   resize_to_default=(w > 0 and h > 0),
                                   upsample_size=4.0)
              if not showBG:
                  image = np.zeros(image.shape)
  
              image = TfPoseEstimator.draw_humans(image, humans, imgcopy=False)
              npimg = np.copy(image)
              image_h, image_w = npimg.shape[:2]
              centers = {}
              
              for human in humans:
                      for i in range(common.CocoPart.Background.value):
                              if i not in human.body_parts.keys():
                                      continue
                              body_part = human.body_parts[i]
                              x_axis=int(body_part.x * image_w + 0.5)
                              y_axis=int(body_part.y * image_h + 0.5) 
                              center=[x_axis,y_axis]
                              centers[i] = center
                      # 수정 : 한 묶음이 append 되도록 수정
                      keypoints_list.append(centers)
  
              cv2.putText(image, "FPS: %f" % (1.0 / (time.time() - prev_time)), (10, 10),
                          cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
              
              cv2.imshow('Dancer', image) 
  
              prev_time = time.time() 
  
              if cv2.waitKey(1) & 0xFF == ord('q'):
                  break
            
          elif (ret_val is False) :
              break
          else:
              # cv2.waitKey(1) => 데이터 분석 안할 때는 delay 초 만큼 지연되도록 수정
              image = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)
              cv2.imshow('Dancer', image) 
              cv2.waitKey(delay)
              
      print(len(keypoints_list))
      cap.release()
      cv2.destroyAllWindows()
      return keypoints_list
  ```

  

##### d. 댄서 자세 데이터 csv 입출력

- 저장

  ```python
  data.to_csv("dancer_keyp_list.csv", index=False, header=False)
  ```

- 읽기

  ```python
  keyp_list=np.genfromtxt("dancer_keyp_list.csv",delimiter=",")
  ```

  

##### e. FPS 수정

- 실제 예상 프레임 수와 약간 차이가 생김
  - 이전엔 괜찮았는데, 갑자기 210->190 정도로 줄어듦
  - 분석할 때 실제 FPS보다 느려서 생기는 딜레이같아서 FPS를 5로 줄이니 105로 예상 프레임 수와 같아짐.



##### f. 웹소켓 종료 후 클라이언트에서 setInterval 종료

- 서버에서 댄서영상이 끝나면 `close` 에러가 계속 뜸 → 이거 수정

  ```html
  <!--videosender.html-->
  <!DOCTYPE html>
  <html>
  <head>
  	<title>Hello</title>
  </head>
  <body>
  	<video id="videoInput" style="display:none"></video>
    <img id="my_video" width="200" src="">
    <video id="dancer_video" width="200" controls muted="muted">
      <source src="http://localhost:8000/game/dancer" type="video/mp4"/>
  </video>
    <canvas id="videoOutput"></canvas>
    <button onclick=stream()>Send</button>
  </body>
  <script>
    var w = 320, h = 240;
      var url = "ws://localhost:8000/client"
      var ws = new WebSocket(url);
      var FPS=5;
      let interval;
  
    ws.binaryType = "arraybuffer";
  
    // websocket 연결
  	ws.onopen = function(){
  		console.log("Websocket is connected.");
  	}
  
  
  
    
  	ws.onmessage = function(msg){
  		// console.log(msg.data);
      var arrayBufferView = new Uint8Array( msg.data);
      var blob = new Blob( [ arrayBufferView ], { type: "multipart/x-mixed-replace" } );
      var urlCreator = window.URL || window.webkitURL;
      var imageUrl = urlCreator.createObjectURL( blob );
      var img = document.querySelector( "#my_video" );
      img.src = imageUrl;
  	}
  
  
      ws.onclose = function(event) {
          if (event.wasClean) {
              clearInterval(interval);
              alert(`[close] 커넥션이 정상적으로 종료되었습니다(code=${event.code} reason=${event.reason})`);
          } else {
              // 예시: 프로세스가 죽거나 네트워크에 장애가 있는 경우
              // event.code가 1006이 됩니다.
              alert('[close] 커넥션이 죽었습니다.');
          }
      };
  
  
  	navigator.getUserMedia = navigator.getUserMedia || navigator.webkitGetUserMedia || navigator.mozGetUserMedia;
    var constraints = {audio: false, video: true};
    var video = document.getElementById("videoInput");
    video.width = w;
    video.height = h;
    function successCallback(stream){
    	video.srcObject = stream;
    	video.play();
    }
    
    function errorCallback(error){
     	console.log(error);
    }
    navigator.getUserMedia(constraints, successCallback, errorCallback);
  	var canvas = document.getElementById("videoOutput");				
    canvas.width = w;
    canvas.height = h;
    var ctx = canvas.getContext("2d");
  
    function processImage(){
          ctx.drawImage(video, 0, 0, w, h);
          setTimeout(processImage, 1);
    }
  
    processImage();
  
    function stream(){
      document.getElementById("dancer_video").play();
  
      interval = setInterval(sendImage, 1000/FPS);
    }
  
    function sendImage(){
      var rawData = canvas.toDataURL("image/jpeg", 0.5);
      ws.send(rawData);
    }
  
  
   
  /* later */
  
  
  </script>
  </html>
  ```

  